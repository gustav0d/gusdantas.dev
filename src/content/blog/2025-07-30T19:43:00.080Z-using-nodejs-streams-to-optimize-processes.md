---
slug: 'using-nodejs-streams-to-optimize-processes'
title: 'Using Node.js Streams to optimize processes'
pubDate: 2025-07-30T19:43:00.080Z
draft: true
tags:
  - nodejs
  - nodejs-streams
  - optimization
---

## Introduction

Recently I had to solve an issue that involved exporting a spreadsheet for a specific client. For some unknown reason, the spreadsheet export didn't work as expected. After some days of serious code and `console.log()` investigations (among other debugging techniques), I found out the server was receiving signals to kill (SIGTERM) when executing the export spreadsheet job.

After some more investigation, I found out that the data of that specific company was too large to keep in memory at once - which was an issue for the way that it was being implemented.

Some things needed to change in order to make it work. What if there is a way of fetching smaller chunks of data, process the chunk, incrementally build the spreadsheet and incrementally upload them? Turns out Node.js Streams is a perfect use case for that.

## What is a Stream?

First, let's use the _via negativa_: what's like when we don't use stream? We wait for all the content is ready, then load it all to the memory:

![](/assets/nodejs-streams/all-at-once.png)

As for stream we can think of a constant flow of data. For example - in simpler and non-technical ways -, a constant flow of water. Behold my drawing:

![illustration depicts a small glass being filled with dripping water from a faucet. An arrow points from the full glass to a comically oversized, empty bucket. Text beside the glass reads: When bucket is filled: - Close water flow; - Put all the water in that comically big bucket](/assets/nodejs-streams/comically-big-bucket.png)

And of course, you definitely already heard of streaming platforms: what does it mean when we watch a movie or listen to a song from a streaming platform? YouTube, for example, doesn't need to send you the full video for you to watch - if that was the case, people would probably stop binge watching because boredom would likely send them away -. The server gradually sends data and the client asks and loads them _dynamically_.

In short, streams enables us to work with parts of data (chunk) as soon as they arrive, with no need for the data to be available all at once.

## Understanding Node.js Streams

Node.js comes with four different [types of Streams](https://nodejs.org/api/stream.html#types-of-streams):

- Readable: Read data chunk by chunk as it arrives.
- Writable: Write data chunk by chunk, sending it out gradually.
- Duplex: Can both read and write data, working bidirectionally.
- Transform: Modify or transform the data as it passes through; it is a **Duplex**.

Let's try a simple example before going further: Let's create 100000 users and display them.

The Readable will be the source of the data:

```ts
import { Readable } from 'node:stream';

const readableStream = new Readable({
  read: function () {
    for (let i = 0; i <= 1e5 /* 100000 */; i++) {
      const user = {
        id: Date.now() + i,
        name: `User-${i}`,
      };
      this.push(JSON.stringify(user));
    }
    this.push(null); // signaling the end of stream
  },
});
```

In the code above we:

- Created the readable stream with `Readable`
- Passed the `read` function
- Pushed the user data to the Readable
- Ended the stream by pushing `null`

Then we can just `console.log` the data with a Writable:

```ts
import { Writable } from 'node:stream';

const writableStream = new Writable({
  write(chunk, enconding, cb) {
    console.log('User data:', chunk);
    cb();
  },
});
```

In the code above we used Writable's write function to:

- receive chunk data
- console.log chunk data

Then we can execute the pipeline:

```ts
import { Readable, Writable } from 'node:stream';
import { pipeline } from 'node:stream/promises';

const readableStream = new Readable({
  read: function () {
    for (let i = 0; i <= 1e5 /* 100000 */; i++) {
      const user = {
        id: Date.now() + i,
        name: `User-${i}`,
      };
      this.push(JSON.stringify(user));
    }
    this.push(null); // signaling the end of stream
  },
});

const writableStream = new Writable({
  write(chunk, enconding, cb) {
    console.log('User data:', chunk);
    cb();
  },
});

await pipeline(readableStream, writableStream);
```

VoilÃ¡! Your terminal should get a bit laggy (I recommend maybe trying less than `1e5` times), but it just works!

But what if you want to read data (of JSON, like the current example) and write it in a file (`.csv` for example)? In that case, we'll need a duplex, which is both a Readable and a Writable, but with a little difference: we'll transform the data; so we'll use a specific kind of duplex: a Transformer.

```ts
import { Readable, Transform } from 'node:stream';
import { pipeline } from 'node:stream/promises';
import { createWriteStream } from 'node:fs';

/* Same readable as before... */

const transformToCsv = new Transform({
  transform: function (chunk, encoding, cb) {
    const data = JSON.parse(chunk);
    const dataToCsvLine = `${data.id},${data.name}\n`;

    cb(null, dataToCsvLine);
  },
});

await pipeline(readableStream, transformToCsv, createWriteStream('users.csv'));
```

The flow here is simple, we receive the chunk in JSON format and transform it to csv format.

But if you execute it, you might realize there's no header. Well... how to fix it? With streams as well! Let's create a `setHeader` Transform that adds the header before the first execution:

```ts
const setHeader = (() => {
  let headerSent = false;
  return new Transform({
    transform(chunk, encoding, cb) {
      if (headerSent) {
        return cb(null, chunk);
      }
      headerSent = true;
      cb(null, 'id,name\n'.concat(chunk));
    },
  });
})();

await pipeline(
  readableStream,
  transformToCsv,
  setHeader,
  createWriteStream('users.csv'),
);
```

## How I actually fixed the issue

- Generating xlsx with exceljs
- adding a sheet dynamically based on Mongoose `cursor().pipe(transformStream)`
  - don't forget to destroy in case of error!
- Amazon S3's upload receives body as Readable stream
- Waiting to finalize operations
- Done!

## Conclusion
