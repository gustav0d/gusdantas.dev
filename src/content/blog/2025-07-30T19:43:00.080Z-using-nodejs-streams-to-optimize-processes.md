---
slug: 'using-nodejs-streams-to-optimize-processes'
title: 'Using Node.js Streams to optimize processes'
pubDate: 2025-07-30T19:43:00.080Z
draft: true
tags:
  - nodejs
  - nodejs-streams
  - optimization
---

## Introduction

Recently I had to solve an issue that involved exporting a spreadsheet for a specific client. For some unknown reason, the spreadsheet export didn't work as expected. After some days of serious code and `console.log()` investigations (among other debugging techniques), I found out the server was receiving signals to kill (SIGTERM) when executing the export spreadsheet job.

After some more investigation, I found out that the data of that specific company was too large to keep in memory at once - which was an issue for the way that it was being implemented.

Some things needed to change in order to make it work. What if there is a way of fetching smaller chunks of data, process the chunk, incrementally build the spreadsheet and incrementally upload them? Turns out Node.js Streams is a perfect use case for that.

## What is a Stream?

First, let's use the _via negativa_: what's like when we don't use stream? We wait for all the content is ready, then load it all to the memory:

![](/assets/nodejs-streams/all-at-once.png)

As for stream we can think of a constant flow of data. For example - in simpler and non-technical ways -, a constant flow of water. Behold my drawing:

![illustration depicts a small glass being filled with dripping water from a faucet. An arrow points from the full glass to a comically oversized, empty bucket. Text beside the glass reads: When bucket is filled: - Close water flow; - Put all the water in that comically big bucket](/assets/nodejs-streams/comically-big-bucket.png)

And of course, you definitely already heard of streaming platforms: what does it mean when we watch a movie or listen to a song from a streaming platform? YouTube, for example, doesn't need to send you the full video for you to watch - if that was the case, people would probably stop binge watching because boredom would likely send them away -. The server gradually sends data and the client asks and loads them _dynamically_.

In short, streams enables us to work with parts of data (chunk) as soon as they arrive, with no need for the data to be available all at once.

## Understanding Node.js Streams

Node.js comes with four different [types of Streams](https://nodejs.org/api/stream.html#types-of-streams):

- Readable: Read data chunk by chunk as it arrives.
- Writable: Write data chunk by chunk, sending it out gradually.
- Duplex: Can both read and write data, working bidirectionally.
- Transform: Modify or transform the data as it passes through; it is a **Duplex**.
