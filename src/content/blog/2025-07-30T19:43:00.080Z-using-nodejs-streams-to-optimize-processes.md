---
slug: 'using-nodejs-streams-to-optimize-processes'
title: 'Using Node.js Streams to optimize processes'
pubDate: 2025-07-30T19:43:00.080Z
draft: true
tags:
  - nodejs
  - nodejs-streams
  - optimization
---

## Introduction

Recently I had to solve an issue that involved exporting a spreadsheet for a specific client. For some unknown reason, the spreadsheet export didn't work as expected. After some days of serious code and `console.log()` investigations (among other debugging techniques), I found out the server was receiving signals to kill (SIGTERM) when executing the export spreadsheet job.

After some more investigation, I found out that the data of that specific company was too large to keep in memory at once - which was an issue for the way that it was being implemented.

Some things needed to change in order to make it work. What if there is a way of fetching smaller chunks of data, process the chunk, incrementally build the spreadsheet and incrementally upload them? Turns out Node.js Streams is a perfect use case for that.

## What is a Stream?

First, let's use the _via negativa_: what's like when we don't use stream? We wait for all the content is ready, then load it all to the memory:

<img src="/assets/nodejs-streams/all-at-once.png" />

Let's think of the work "stream" for a second. What does it mean when we watch a movie or listen to a song from a streaming platform instead of downloading the content? YouTube, for example, doesn't need to send you the full video for you to watch - if that was the case, people would probably stop binge watching because boredom would likely send them away -. The server gradually sends data and the client asks and loads them dynamically.
